{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-17T14:04:42.252778Z",
     "start_time": "2024-05-17T14:04:41.690334700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from trino.dbapi import connect \n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "outputs": [],
   "source": [
    "# def read_from_hive(sql_script):\n",
    "#     conn = connect(\n",
    "#             host='presto.bstis.com',\n",
    "#             port=8080,\n",
    "#             user='hadoop',\n",
    "#             catalog='hive',\n",
    "#             #schema='default'\n",
    "#             )\n",
    "#     cur = conn.cursor()\n",
    "#     cur.execute(sql_script)\n",
    "# \n",
    "#     columns = [desc[0] for desc in cur.description]\n",
    "#     rows = cur.fetchall()  # Fetch all rows at once\n",
    "# \n",
    "#     df = pd.DataFrame(rows, columns=columns)\n",
    "# \n",
    "#     cur.close()\n",
    "#     conn.close()\n",
    "# \n",
    "#     return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T14:04:42.261747800Z",
     "start_time": "2024-05-17T14:04:42.077468800Z"
    }
   },
   "id": "fb4db003d6a9dcb5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Claims Raw"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "987e8bc9809bdd49"
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "outputs": [],
   "source": [
    "# def fetch_monthly_data(start_date, end_date):\n",
    "#     sSQL = f'''\n",
    "#     select\n",
    "#     division as tenantid, \n",
    "#     paiddate, \n",
    "#     claimtype, \n",
    "#     icd9_1_chapter, \n",
    "#    -- facility_indicator, \n",
    "#    -- revenue, \n",
    "#    -- cpt_code, \n",
    "#     servicecategory_details, \n",
    "#    -- provider_pcp_flag, \n",
    "#    -- providerspecialty, \n",
    "#    -- reversalstatus, \n",
    "#     personid, \n",
    "#     tpaclaimid, \n",
    "#     amtallowed, \n",
    "#     amtpaid, \n",
    "#    -- amtdeductible, \n",
    "#    -- amtcoins, \n",
    "#    -- amtcopay, \n",
    "#    -- amtcobpaid, \n",
    "#    -- amthra, \n",
    "#    -- amtpaidbyothersins, \n",
    "#     locationtype, \n",
    "#     locationtypedesc, \n",
    "#     therapeutic_class_name, \n",
    "#     tag_tpa, \n",
    "#     tag_exchange, \n",
    "#     claimid\n",
    "# \n",
    "#     from hive.trinet.claims c\n",
    "# \n",
    "#     where  paiddate <= date('{end_date}')\n",
    "#      AND paiddate >= date('{start_date}')\n",
    "#      and claimtype in ('MED', 'Rx')\n",
    "# \n",
    "#     '''\n",
    "# \n",
    "#     df_claims = read_from_hive(sSQL)\n",
    "#     df_claims.to_csv(f'Claims Data/claims_raw_{start_date.replace(\"-\", \"_\")}.gz',index=False, compression='gzip')\n",
    "# \n",
    "# # Define the start and end dates\n",
    "# start_date = pd.to_datetime('2023-04-01')\n",
    "# end_date = pd.to_datetime('2023-04-30')\n",
    "# \n",
    "# # Loop over each month\n",
    "# while start_date <= end_date:\n",
    "#     next_month = start_date + relativedelta(months=1)\n",
    "#     fetch_monthly_data(start_date.strftime('%Y-%m-%d'), (next_month - pd.Timedelta(days=1)).strftime('%Y-%m-%d'))\n",
    "#     start_date = next_month"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T14:04:42.261747800Z",
     "start_time": "2024-05-17T14:04:42.077468800Z"
    }
   },
   "id": "a56866a7c85ffe5e"
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "outputs": [],
   "source": [
    "# sSQL = '''\n",
    "# select \n",
    "# claimid, inpatient_visitid, er_visitid, med_visitid\n",
    "#  from hive.trinet.claims_inpatient\n",
    "# where inpatient_visitid <> '' or er_visitid <> ''\n",
    "# \n",
    "# '''\n",
    "# \n",
    "# df_claims = read_from_hive(sSQL)\n",
    "# del sSQL\n",
    "# \n",
    "# df_claims.to_csv('Claims Data/claimsip_raw.gz',index=False, compression='gzip')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T14:04:42.261747800Z",
     "start_time": "2024-05-17T14:04:42.077468800Z"
    }
   },
   "id": "fe70ab6abbfff136"
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "outputs": [],
   "source": [
    "# def fetch_monthly_ccmap(start_date, end_date):\n",
    "#     sSQL = f'''\n",
    "#     select * \n",
    "#     from hive.trinet.ccmap\n",
    "# \n",
    "#     where  date <= date('{end_date}')\n",
    "#      AND date >= date('{start_date}')\n",
    "#     '''\n",
    "# \n",
    "#     df_ccmap = read_from_hive(sSQL)\n",
    "#     df_ccmap.to_csv(f'Claims Data/ccmap_raw_{start_date.replace(\"-\", \"_\")}.gz',index=False, compression='gzip')\n",
    "# \n",
    "# # Define the start and end dates\n",
    "# start_date = pd.to_datetime('2021-10-01')\n",
    "# end_date = pd.to_datetime('2024-03-31')\n",
    "# \n",
    "# # Loop over each month\n",
    "# while start_date <= end_date:\n",
    "#     next_month = start_date + relativedelta(months=1)\n",
    "#     fetch_monthly_ccmap(start_date.strftime('%Y-%m-%d'), (next_month - pd.Timedelta(days=1)).strftime('%Y-%m-%d'))\n",
    "#     start_date = next_month"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T14:04:42.261747800Z",
     "start_time": "2024-05-17T14:04:42.078448800Z"
    }
   },
   "id": "a1d95a2ae49ee5a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set Period"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "231abe968cddae3b"
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "outputs": [],
   "source": [
    "def calculate_dates(input_date):\n",
    "    curr_month = pd.to_datetime(input_date).strftime('%Y-%m-%d')\n",
    "    stop_date = (pd.to_datetime(curr_month) + pd.DateOffset(months=1) - pd.DateOffset(days=1)).strftime('%Y-%m-%d')\n",
    "    start_date = (pd.to_datetime(stop_date) - pd.DateOffset(months=12) + pd.DateOffset(days=1)).strftime('%Y-%m-%d')\n",
    "    return curr_month, stop_date, start_date\n",
    "\n",
    "curr_month, stop_date, start_date = calculate_dates('2022-11-01')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T14:04:42.261747800Z",
     "start_time": "2024-05-17T14:04:42.078448800Z"
    }
   },
   "id": "8f5b0eb4eeba23a3"
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "outputs": [],
   "source": [
    "def calculate_dates(curr_month):\n",
    "    curr_month = pd.to_datetime(curr_month)\n",
    "    stop_date_nonkaiser = (curr_month + pd.DateOffset(months=1) - pd.DateOffset(days=1))\n",
    "    start_date_nonkaiser = (stop_date_nonkaiser - pd.DateOffset(months=12) + pd.DateOffset(days=1))\n",
    "    stop_date_kaiser = (stop_date_nonkaiser - pd.DateOffset(months=1))\n",
    "    start_date_kaiser = (start_date_nonkaiser - pd.DateOffset(months=1))\n",
    "    \n",
    "    return stop_date_nonkaiser, start_date_nonkaiser, stop_date_kaiser, start_date_kaiser\n",
    "\n",
    "stop_date_nonkaiser, start_date_nonkaiser, stop_date_kaiser, start_date_kaiser = calculate_dates(curr_month)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-17T14:04:42.261747800Z",
     "start_time": "2024-05-17T14:04:42.201948200Z"
    }
   },
   "id": "69a55bedc982e8a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-process Claims"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "606de4082844e392"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_data(date, is_kaiser):\n",
    "    file_name = f'Claims Data/claims_raw_{date.strftime(\"%Y_%m_%d\")}.gz'\n",
    "    df = pd.read_csv(file_name, compression='gzip',\n",
    "                     dtype={'tenantid':str,\n",
    "                            'personid':str,\n",
    "                            'claimtype':str,\n",
    "                            'icd9_1_chapter':str,\n",
    "                            'servicecategory_details':str,\n",
    "                            'locationtype':str,\n",
    "                            'locationtypedesc':str,\n",
    "                            'therapeutic_class_name':str,\n",
    "                            'tag_tpa':str,\n",
    "                            'tag_exchange':str,\n",
    "                            'claimid':str,\n",
    "                            'amtpaid':np.float64,\n",
    "                            'amtallowed':np.float64,\n",
    "                            'tpaclaimid':str})\n",
    "    if is_kaiser:\n",
    "        return df[df['tag_tpa'] == 'kaiser']\n",
    "    else:\n",
    "        return df[df['tag_tpa'] != 'kaiser']\n",
    "\n",
    "def read_claims_data(start_date, end_date, is_kaiser):\n",
    "    dfs = []\n",
    "    while start_date <= end_date:\n",
    "        dfs.append(read_data(start_date, is_kaiser))\n",
    "        start_date += relativedelta(months=1)\n",
    "    df = pd.concat(dfs, axis=0)\n",
    "    return df\n",
    "\n",
    "df_claims_nonkaiser = read_claims_data(start_date_nonkaiser, stop_date_nonkaiser, False)\n",
    "df_claims_kaiser = read_claims_data(start_date_kaiser, stop_date_kaiser, True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-17T14:04:42.287663200Z"
    }
   },
   "id": "50e9387a504044fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_all_claims = pd.concat([df_claims_nonkaiser, df_claims_kaiser], axis=0)\n",
    "del df_claims_nonkaiser, df_claims_kaiser\n",
    "\n",
    "df_claims = df_all_claims.query(\"~((tag_tpa == 'bcbsmn' & tag_exchange == 'TriNet III') | \"\n",
    "                                \"(tag_tpa == 'kaiser' & tag_exchange == 'TriNet IV') | \"\n",
    "                                \"(tag_tpa == 'excellus' & tag_exchange == 'TriNet I'))\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9656f021fbf92f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Catastrophic"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1afa959d10c7509a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cat = (df_claims[['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'amtallowed', 'amtpaid', 'tpaclaimid']]\n",
    "          .groupby(['tenantid', 'tag_tpa', 'tag_exchange', 'personid'])\n",
    "          .agg({'amtallowed':'sum', 'amtpaid':'sum', 'tpaclaimid':'count'})\n",
    "          .rename(columns={'tpaclaimid':'util'})\n",
    "          .reset_index()\n",
    "          .assign(**{'class':'Catastrophic'})\n",
    "          .loc[lambda df: df['amtpaid'] >= 100000])\n",
    "\n",
    "# df_cat"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10c7211842619f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cat_chapter = (df_claims[['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'icd9_1_chapter', 'amtallowed', 'amtpaid', 'tpaclaimid']]\n",
    "                  .groupby(['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'icd9_1_chapter'])\n",
    "                  .agg({'amtallowed':'sum', 'amtpaid':'sum', 'tpaclaimid':'nunique'})\n",
    "                  .reset_index()\n",
    "                  .rename(columns={'tpaclaimid':'claim_count'})\n",
    "                  .merge(df_cat[['tenantid', 'tag_tpa', 'tag_exchange', 'personid']], on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='right'))\n",
    "# df_cat_chapter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea7cd89635a07428"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cat_rx_chapter = (df_claims[['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'therapeutic_class_name', 'amtallowed', 'amtpaid', 'tpaclaimid']]\n",
    "                     .groupby(['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'therapeutic_class_name'])\n",
    "                     .agg({'amtallowed':'sum', 'amtpaid':'sum', 'tpaclaimid':'nunique'})\n",
    "                     .reset_index()\n",
    "                     .rename(columns={'tpaclaimid':'claim_count'})\n",
    "                     .merge(df_cat[['tenantid', 'tag_tpa', 'tag_exchange', 'personid']], on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='right'))\n",
    "# df_cat_rx_chapter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ddeb9ba48b70733"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cat_max = (df_cat_chapter.copy()\n",
    "              .sort_values(by=['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'amtallowed'], ascending=[True,True, True, True,False])\n",
    "              .groupby(['tenantid', 'tag_tpa', 'tag_exchange', 'personid'])\n",
    "              .first()\n",
    "              .reset_index())\n",
    "# df_cat_max"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea8156b89f8a878"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cat_rx_max = (df_cat_rx_chapter.copy()\n",
    "                 .sort_values(by=['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'amtallowed'], ascending=[True,True, True, True,False])\n",
    "                 .groupby(['tenantid', 'tag_tpa', 'tag_exchange', 'personid'])\n",
    "                 .first()\n",
    "                 .reset_index()\n",
    "                 .assign(therapeutic_class_name = lambda df: df['therapeutic_class_name'].str.title()))\n",
    "# df_cat_rx_max"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "765b40dd3f72faf5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cat_icd9chapter = (df_cat_max.merge(df_cat_rx_max, how='outer', on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], suffixes=('_a', '_b'))\n",
    "                      .assign(icd9_1_chapter = lambda df: df['icd9_1_chapter'].where(df['icd9_1_chapter'].notna(), df['therapeutic_class_name']))\n",
    "                      .loc[:, ['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'icd9_1_chapter']]\n",
    "                      .assign(**{'class':'Catastrophic'}))\n",
    "# df_cat_icd9chapter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22a91b11aa80bb49"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cat_person = pd.merge(df_cat_icd9chapter, df_cat, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'class'], how='left')\n",
    "# df_cat_person"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17e7b4d9cd06c576"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cat_groups = (df_cat_person.copy()\n",
    "                 .loc[:, ['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'class', 'icd9_1_chapter']]\n",
    "                 .rename(columns={'icd9_1_chapter':'group'}))\n",
    "# df_cat_groups"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64ddcd73718efe14"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del df_cat_chapter, df_cat_rx_chapter, df_cat_max, df_cat_rx_max, df_cat_icd9chapter, df_cat_person"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad1dfefb46dcdfed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Chronic"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5ff5859fdf7e137"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_carrier_nonkaiser = (df_claims[['tenantid', 'personid', 'tag_tpa', 'tag_exchange']]\n",
    "                        .drop_duplicates()\n",
    "                        .query(\"tag_tpa != 'kaiser'\"))\n",
    "\n",
    "df_carrier_kaiser = (df_claims[['tenantid', 'personid', 'tag_tpa', 'tag_exchange']]\n",
    "                     .drop_duplicates()\n",
    "                     .query(\"tag_tpa == 'kaiser'\"))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7433629124f00ce1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stop_date_nonkaiser = pd.to_datetime((pd.to_datetime(curr_month) + pd.DateOffset(months=1) - pd.DateOffset(days=1)).strftime('%Y-%m-%d'))\n",
    "start_date_nonkaiser = pd.to_datetime((pd.to_datetime(stop_date_nonkaiser) - pd.DateOffset(months=12) + pd.DateOffset(days=1)).strftime('%Y-%m-%d'))\n",
    "stop_date_kaiser = pd.to_datetime((pd.to_datetime(stop_date_nonkaiser) - pd.DateOffset(months=1)).strftime('%Y-%m-%d'))\n",
    "start_date_kaiser = pd.to_datetime((pd.to_datetime(start_date_nonkaiser) - pd.DateOffset(months=1)).strftime('%Y-%m-%d'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2ee18bc948a1150"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_nonkaiser_ccmap(start_date_nonkaiser, stop_date_nonkaiser):\n",
    "    file_name = f'Claims Data/ccmap_raw_{start_date_nonkaiser.replace(\"-\", \"_\")}.gz'\n",
    "    df_ccmap_nonkaiser = pd.read_csv(file_name, compression='gzip', dtype={'tenantid':str, 'personid':str})\n",
    "    return df_ccmap_nonkaiser\n",
    "\n",
    "# Initialize an empty DataFrame to hold all the data\n",
    "df_ccmap_nonkaiser = pd.DataFrame()\n",
    "\n",
    "# Loop over each month\n",
    "while start_date_nonkaiser <= stop_date_nonkaiser:\n",
    "    next_month = start_date_nonkaiser + relativedelta(months=1)\n",
    "    df_monthly_ccmap_nonkaiser = read_nonkaiser_ccmap(start_date_nonkaiser.strftime('%Y-%m-%d'), (next_month - pd.Timedelta(days=1)).strftime('%Y-%m-%d'))\n",
    "    df_ccmap_nonkaiser = pd.concat([df_ccmap_nonkaiser, df_monthly_ccmap_nonkaiser], axis=0)\n",
    "    start_date_nonkaiser = next_month\n",
    "\n",
    "\n",
    "df_ccmap_nonkaiser.drop(['tenantid'], axis=1, inplace=True)\n",
    "df_ccmap_nonkaiser = pd.merge(df_carrier_nonkaiser, df_ccmap_nonkaiser, left_on=['tenantid','personid'], right_on=['division','personid'], how='left')\n",
    "df_ccmap_nonkaiser.dropna(subset=['ccondition'], inplace=True)\n",
    "# df_ccmap_nonkaiser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8095a85b737a6b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_kaiser_ccmap(start_date_kaiser, stop_date_kaiser):\n",
    "    file_name = f'Claims Data/ccmap_raw_{start_date_kaiser.replace(\"-\", \"_\")}.gz'\n",
    "    df_ccmap_kaiser = pd.read_csv(file_name, compression='gzip', dtype={'tenantid':str, 'personid':str})\n",
    "    return df_ccmap_kaiser\n",
    "\n",
    "# Initialize an empty DataFrame to hold all the data\n",
    "df_ccmap_kaiser = pd.DataFrame()\n",
    "\n",
    "# Loop over each month\n",
    "while start_date_kaiser <= stop_date_kaiser:\n",
    "    next_month = start_date_kaiser + relativedelta(months=1)\n",
    "    df_monthly_ccmap_kaiser = read_kaiser_ccmap(start_date_kaiser.strftime('%Y-%m-%d'), (next_month - pd.Timedelta(days=1)).strftime('%Y-%m-%d'))\n",
    "    df_ccmap_kaiser = pd.concat([df_ccmap_kaiser, df_monthly_ccmap_kaiser], axis=0)\n",
    "    start_date_kaiser = next_month\n",
    "\n",
    "df_ccmap_kaiser.drop(['tenantid'], axis=1, inplace=True)\n",
    "# df_ccmap_kaiser.rename(columns={'division':'tenantid'})\n",
    "df_ccmap_kaiser = pd.merge(df_carrier_kaiser, df_ccmap_kaiser, left_on=['tenantid','personid'], right_on=['division','personid'], how='left')\n",
    "df_ccmap_kaiser.dropna(subset=['ccondition'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b228fe8cd5164a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ccmap = pd.concat([df_ccmap_nonkaiser, df_ccmap_kaiser], axis=0)\n",
    "del df_ccmap_nonkaiser, df_ccmap_kaiser\n",
    "# df_ccmap"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1981a51f3a94a5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def update_ccmap(df_ccmap, df_cat, condition_file):\n",
    "    df_updated_condition = pd.read_csv(condition_file)\n",
    "    df_ccmap = pd.merge(df_ccmap, df_cat[['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'amtallowed']],\n",
    "                        on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "    df_ccmap = df_ccmap[df_ccmap['amtallowed'].isna()]\n",
    "    df_ccmap = pd.merge(df_ccmap, df_updated_condition, on=['ccondition_category'], how='left')\n",
    "    return df_ccmap\n",
    "\n",
    "df_updated_ccmap = update_ccmap(df_ccmap, df_cat, 'updated_condition.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "187d4b45c2e2322c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ccmap_count = (df_updated_ccmap.groupby(['tenantid', 'tag_tpa', 'tag_exchange', 'personid'])\n",
    "                  .agg({'updated_category':'nunique'})\n",
    "                  .rename(columns={'updated_category':'condition_count'})\n",
    "                  .reset_index())\n",
    "\n",
    "# df_ccmap_count"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "963570d46238aca4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter for 'cancer', select columns, and drop duplicates\n",
    "df_ccmap_cancer_all = (df_updated_ccmap[df_updated_ccmap['updated_category'].str.lower() == 'cancer']\n",
    "                       [['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "                       .drop_duplicates())\n",
    "\n",
    "# Merge with df_ccmap_count\n",
    "df_ccmap_cancer_all = df_ccmap_cancer_all.merge(df_ccmap_count, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "\n",
    "# Create 'cancer only' and 'cancer complex' groups\n",
    "df_ccmap_cancer_only = df_ccmap_cancer_all.loc[df_ccmap_cancer_all['condition_count'] == 1].assign(group='cancer only')\n",
    "df_ccmap_cancer_comorb = df_ccmap_cancer_all.loc[df_ccmap_cancer_all['condition_count'] != 1].assign(group='cancer complex')\n",
    "\n",
    "# df_ccmap_cancer_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9788abed280ee708"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter for 'heart disease', select columns, and drop duplicates\n",
    "df_ccmap_cardiac_all = (df_updated_ccmap[df_updated_ccmap['updated_category'].str.lower() == 'heart disease']\n",
    "                        [['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "                        .drop_duplicates())\n",
    "\n",
    "# Merge with df_ccmap_cancer_all and df_ccmap_count, and filter rows\n",
    "df_ccmap_cardiac_all = (df_ccmap_cardiac_all\n",
    "                        .merge(df_ccmap_cancer_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "                        .loc[lambda df: df['condition_count'].isna()]\n",
    "                        .drop('condition_count', axis=1)\n",
    "                        .merge(df_ccmap_count, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left'))\n",
    "\n",
    "# Create 'heart disease only' and 'heart disease complex' groups\n",
    "df_ccmap_cardiac_only = df_ccmap_cardiac_all.loc[df_ccmap_cardiac_all['condition_count'] == 1].assign(group='heart disease only')\n",
    "df_ccmap_cardiac_comorb = df_ccmap_cardiac_all.loc[df_ccmap_cardiac_all['condition_count'] != 1].assign(group='heart disease complex')\n",
    "\n",
    "# df_ccmap_cardiac_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ed7f6010fb9e91c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter for 'chronic kidney disease', select columns, and drop duplicates\n",
    "df_ccmap_ckd_all = (df_updated_ccmap[df_updated_ccmap['updated_category'].str.lower() == 'chronic kidney disease']\n",
    "                    [['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "                    .drop_duplicates())\n",
    "\n",
    "# Merge with df_ccmap_cancer_all and df_ccmap_cardiac_all, and filter rows\n",
    "df_ccmap_ckd_all = (df_ccmap_ckd_all\n",
    "                    .merge(df_ccmap_cancer_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_a', '_b'))\n",
    "                    .merge(df_ccmap_cardiac_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_c', '_d'))\n",
    "                    .loc[lambda df: df.filter(like='condition_count').isna().all(axis=1)])\n",
    "\n",
    "# Drop columns that contain 'condition_count' in their names\n",
    "df_ccmap_ckd_all = df_ccmap_ckd_all.drop(df_ccmap_ckd_all.filter(like='condition_count').columns, axis=1)\n",
    "\n",
    "# Merge with df_ccmap_count\n",
    "df_ccmap_ckd_all = df_ccmap_ckd_all.merge(df_ccmap_count, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "\n",
    "# Create 'CKD only' and 'CKD complex' groups\n",
    "df_ccmap_ckd_only = df_ccmap_ckd_all.loc[df_ccmap_ckd_all['condition_count'] == 1].assign(group='CKD only')\n",
    "df_ccmap_ckd_comorb = df_ccmap_ckd_all.loc[df_ccmap_ckd_all['condition_count'] != 1].assign(group='CKD complex')\n",
    "\n",
    "# df_ccmap_ckd_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2f3730e4a63e978"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter for 'lung disease', select columns, and drop duplicates\n",
    "df_ccmap_lung_all = (df_updated_ccmap[df_updated_ccmap['updated_category'].str.lower() == 'lung disease']\n",
    "                     [['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "                     .drop_duplicates())\n",
    "\n",
    "# Merge with df_ccmap_cancer_all, df_ccmap_cardiac_all, and df_ccmap_ckd_all, and filter rows\n",
    "df_ccmap_lung_all = (df_ccmap_lung_all\n",
    "                     .merge(df_ccmap_cancer_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_a', '_b'))\n",
    "                     .merge(df_ccmap_cardiac_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_c', '_d'))\n",
    "                     .merge(df_ccmap_ckd_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_e', '_f'))\n",
    "                     .loc[lambda df: df.filter(like='condition_count').isna().all(axis=1)])\n",
    "\n",
    "# Drop columns that contain 'condition_count' in their names\n",
    "df_ccmap_lung_all = df_ccmap_lung_all.drop(df_ccmap_lung_all.filter(like='condition_count').columns, axis=1)\n",
    "\n",
    "# Merge with df_ccmap_count\n",
    "df_ccmap_lung_all = df_ccmap_lung_all.merge(df_ccmap_count, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "\n",
    "# Create 'lung disease only' and 'lung disease complex' groups\n",
    "df_ccmap_lung_only = df_ccmap_lung_all.loc[df_ccmap_lung_all['condition_count'] == 1].assign(group='lung disease only')\n",
    "df_ccmap_lung_comorb = df_ccmap_lung_all.loc[df_ccmap_lung_all['condition_count'] != 1].assign(group='lung disease complex')\n",
    "# df_ccmap_lung_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f43ef32a50df2fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter for 'mental, behavioral and neurological', select columns, and drop duplicates\n",
    "df_ccmap_mental_all = (df_updated_ccmap[df_updated_ccmap['updated_category'].str.lower() == 'mental, behavioral and neurological']\n",
    "                       [['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "                       .drop_duplicates())\n",
    "\n",
    "# Merge with df_ccmap_cancer_all, df_ccmap_cardiac_all, df_ccmap_ckd_all, and df_ccmap_lung_all, and filter rows\n",
    "df_ccmap_mental_all = (df_ccmap_mental_all\n",
    "                       .merge(df_ccmap_cancer_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_a', '_b'))\n",
    "                       .merge(df_ccmap_cardiac_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_c', '_d'))\n",
    "                       .merge(df_ccmap_ckd_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_e', '_f'))\n",
    "                       .merge(df_ccmap_lung_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_g', '_h'))\n",
    "                       .loc[lambda df: df.filter(like='condition_count').isna().all(axis=1)])\n",
    "\n",
    "# Drop columns that contain 'updated_category' in their names\n",
    "df_ccmap_mental_all = df_ccmap_mental_all.drop(df_ccmap_mental_all.filter(like='condition_count').columns, axis=1)\n",
    "\n",
    "# Merge with df_ccmap_count\n",
    "df_ccmap_mental_all = df_ccmap_mental_all.merge(df_ccmap_count, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "\n",
    "# Create 'mental health only' and 'mental health complex' groups\n",
    "df_ccmap_mental_only = df_ccmap_mental_all.loc[df_ccmap_mental_all['condition_count'] == 1].assign(group='mental health only')\n",
    "df_ccmap_mental_comorb = df_ccmap_mental_all.loc[df_ccmap_mental_all['condition_count'] != 1].assign(group='mental health complex')\n",
    "# df_ccmap_mental_all\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "996424de2b62cce7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter for 'diabetes', select columns, drop duplicates, and merge with other DataFrames\n",
    "df_ccmap_diabetes_all = (df_updated_ccmap[df_updated_ccmap['updated_category'].str.lower() == 'diabetes']\n",
    "                         [['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "                         .drop_duplicates())\n",
    "# Merge with df_ccmap_cancer_all, df_ccmap_cardiac_all, df_ccmap_ckd_all, df_ccmap_lung_all, and df_ccmap_mental_all, and filter rows\n",
    "df_ccmap_diabetes_all = (df_ccmap_diabetes_all\n",
    "                         .merge(df_ccmap_cancer_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_a', '_b'))\n",
    "                         .merge(df_ccmap_cardiac_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_c', '_d'))\n",
    "                         .merge(df_ccmap_ckd_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_e', '_f'))\n",
    "                         .merge(df_ccmap_lung_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_g', '_h'))\n",
    "                         .merge(df_ccmap_mental_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_i', '_j'))\n",
    "                         .loc[lambda df: df.filter(like='condition_count').isna().all(axis=1)])\n",
    "\n",
    "# Drop columns that contain 'condition_count' in their names\n",
    "df_ccmap_diabetes_all = df_ccmap_diabetes_all.drop(df_ccmap_diabetes_all.filter(like='condition_count').columns, axis=1)\n",
    "\n",
    "# Merge with df_ccmap_count\n",
    "df_ccmap_diabetes_all = df_ccmap_diabetes_all.merge(df_ccmap_count, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "\n",
    "# Create 'diabetes only' and 'diabetes complex' groups\n",
    "df_ccmap_diabetes_only = df_ccmap_diabetes_all.loc[df_ccmap_diabetes_all['condition_count'] == 1].assign(group='diabetes only')\n",
    "df_ccmap_diabetes_comorb = df_ccmap_diabetes_all.loc[df_ccmap_diabetes_all['condition_count'] != 1].assign(group='diabetes complex')\n",
    "# df_ccmap_diabetes_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4d621ddffdc107a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter for 'metabolic', select columns, drop duplicates, and merge with other DataFrames\n",
    "df_ccmap_metabolic_all = (df_updated_ccmap[df_updated_ccmap['updated_category'].str.lower() == 'endocrine, nutritional, digestive and metabolic']\n",
    "                         [['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "                         .drop_duplicates())\n",
    "\n",
    "# Merge with df_ccmap_cancer_all, df_ccmap_cardiac_all, df_ccmap_ckd_all, df_ccmap_lung_all, df_ccmap_mental_all, and df_ccmap_diabetes_all, and filter rows\n",
    "df_ccmap_metabolic_all = (df_ccmap_metabolic_all\n",
    "                         .merge(df_ccmap_cancer_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_a', '_b'))\n",
    "                         .merge(df_ccmap_cardiac_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_c', '_d'))\n",
    "                         .merge(df_ccmap_ckd_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_e', '_f'))\n",
    "                         .merge(df_ccmap_lung_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_g', '_h'))\n",
    "                         .merge(df_ccmap_mental_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_i', '_j'))\n",
    "                         .merge(df_ccmap_diabetes_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_k', '_l'))\n",
    "                         .loc[lambda df: df.filter(like='condition_count').isna().all(axis=1)])\n",
    "\n",
    "# Drop columns that contain 'condition_count' in their names\n",
    "df_ccmap_metabolic_all = df_ccmap_metabolic_all.drop(df_ccmap_metabolic_all.filter(like='condition_count').columns, axis=1)\n",
    "\n",
    "# Merge with df_ccmap_count\n",
    "df_ccmap_metabolic_all = df_ccmap_metabolic_all.merge(df_ccmap_count, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "\n",
    "# Create 'metabolic only' and 'metabolic complex' groups\n",
    "df_ccmap_metabolic_only = df_ccmap_metabolic_all.loc[df_ccmap_metabolic_all['condition_count'] == 1].assign(group='metabolic only')\n",
    "df_ccmap_metabolic_comorb = df_ccmap_metabolic_all.loc[df_ccmap_metabolic_all['condition_count'] != 1].assign(group='metabolic complex')\n",
    "# df_ccmap_metabolic_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d1bd4ddea6c0c55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter for 'metabolic', select columns, drop duplicates, and merge with other DataFrames\n",
    "df_ccmap_MSK_all = (df_updated_ccmap[df_updated_ccmap['updated_category'].str.lower() == 'musculoskeletal and injury']\n",
    "                         [['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "                         .drop_duplicates())\n",
    "\n",
    "# Merge with df_ccmap_cancer_all, df_ccmap_cardiac_all, df_ccmap_ckd_all, df_ccmap_lung_all, df_ccmap_mental_all, df_ccmap_diabetes_all, and df_ccmap_metabolic_all, and filter rows\n",
    "df_ccmap_MSK_all = (df_ccmap_MSK_all\n",
    "                         .merge(df_ccmap_cancer_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_a', '_b'))\n",
    "                         .merge(df_ccmap_cardiac_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_c', '_d'))\n",
    "                         .merge(df_ccmap_ckd_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_e', '_f'))\n",
    "                         .merge(df_ccmap_lung_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_g', '_h'))\n",
    "                         .merge(df_ccmap_mental_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_i', '_j'))\n",
    "                         .merge(df_ccmap_diabetes_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_k', '_l'))\n",
    "                         .merge(df_ccmap_metabolic_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_m', '_n'))\n",
    "                         .loc[lambda df: df.filter(like='condition_count').isna().all(axis=1)])\n",
    "\n",
    "# Drop columns that contain 'condition_count' in their names\n",
    "df_ccmap_MSK_all = df_ccmap_MSK_all.drop(df_ccmap_MSK_all.filter(like='condition_count').columns, axis=1)\n",
    "\n",
    "# Merge with df_ccmap_count\n",
    "df_ccmap_MSK_all = df_ccmap_MSK_all.merge(df_ccmap_count, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "\n",
    "# Create 'metabolic only' and 'metabolic complex' groups\n",
    "df_ccmap_MSK_only = df_ccmap_MSK_all.loc[df_ccmap_MSK_all['condition_count'] == 1].assign(group='MSK only')\n",
    "df_ccmap_MSK_comorb = df_ccmap_MSK_all.loc[df_ccmap_MSK_all['condition_count'] != 1].assign(group='MSK complex')\n",
    "# df_ccmap_MSK_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90f805f8b311d841"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ccmap_other_all = (df_updated_ccmap\n",
    "                         [['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "                        .drop_duplicates())\n",
    "\n",
    "# Merge with df_ccmap_cancer_all, df_ccmap_cardiac_all, df_ccmap_ckd_all, df_ccmap_lung_all, df_ccmap_mental_all, df_ccmap_diabetes_all, and df_ccmap_metabolic_all, and filter rows\n",
    "df_ccmap_other_all = (df_ccmap_other_all\n",
    "                         .merge(df_ccmap_cancer_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_a', '_b'))\n",
    "                         .merge(df_ccmap_cardiac_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_c', '_d'))\n",
    "                         .merge(df_ccmap_ckd_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_e', '_f'))\n",
    "                         .merge(df_ccmap_lung_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_g', '_h'))\n",
    "                         .merge(df_ccmap_mental_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_i', '_j'))\n",
    "                         .merge(df_ccmap_diabetes_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_k', '_l'))\n",
    "                         .merge(df_ccmap_metabolic_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_m', '_n'))\n",
    "                         .merge(df_ccmap_MSK_all, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left', suffixes=('_o', '_p'))\n",
    "                         .loc[lambda df: df.filter(like='condition_count').isna().all(axis=1)])\n",
    "\n",
    "# Drop columns that contain 'condition_count' in their names\n",
    "df_ccmap_other_all = df_ccmap_other_all.drop(df_ccmap_other_all.filter(like='condition_count').columns, axis=1)\n",
    "\n",
    "# Merge with df_ccmap_count\n",
    "df_ccmap_other_all = df_ccmap_other_all.merge(df_ccmap_count, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "\n",
    "# Create 'other single' and 'other complex' groups\n",
    "df_ccmap_other_only = df_ccmap_other_all.loc[df_ccmap_other_all['condition_count'] == 1].assign(group='other single')\n",
    "df_ccmap_other_comorb = df_ccmap_other_all.loc[df_ccmap_other_all['condition_count'] != 1].assign(group='other complex')\n",
    "\n",
    "# df_ccmap_other_all"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65e73eef4c84fe7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ccgroups = pd.concat([df_ccmap_cancer_only, df_ccmap_cancer_comorb, df_ccmap_cardiac_only, df_ccmap_cardiac_comorb, df_ccmap_ckd_only, df_ccmap_ckd_comorb, df_ccmap_lung_only, df_ccmap_lung_comorb, df_ccmap_mental_only, df_ccmap_mental_comorb, df_ccmap_diabetes_only, df_ccmap_diabetes_comorb, df_ccmap_metabolic_only, df_ccmap_metabolic_comorb, df_ccmap_MSK_only, df_ccmap_MSK_comorb, df_ccmap_other_only, df_ccmap_other_comorb], axis=0)\n",
    "\n",
    "del df_ccmap_cancer_only, df_ccmap_cancer_comorb, df_ccmap_cardiac_only, df_ccmap_cardiac_comorb, df_ccmap_ckd_only, df_ccmap_ckd_comorb, df_ccmap_lung_only, df_ccmap_lung_comorb, df_ccmap_mental_only, df_ccmap_mental_comorb, df_ccmap_diabetes_only, df_ccmap_diabetes_comorb, df_ccmap_metabolic_only, df_ccmap_metabolic_comorb, df_ccmap_MSK_only, df_ccmap_MSK_comorb, df_ccmap_other_only, df_ccmap_other_comorb\n",
    "\n",
    "df_ccgroups['class'] = 'Chronic Condition'\n",
    "# df_ccgroups"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed7bb0066ef9ba1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Acute"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e40eb9b84b54c9a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_claimsip = pd.read_csv('Claims Data/claimsip_raw.gz', compression='gzip', dtype={'claimid':str})\n",
    "\n",
    "# Create df_acute_er DataFrame\n",
    "df_acute_er = (\n",
    "    df_claims[['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'claimid']]\n",
    "    .merge(df_claimsip, on=['claimid'], how='left')\n",
    "    .query(\"`er_visitid`.notna()\")\n",
    "    .loc[:, ['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "    .drop_duplicates()\n",
    "    .merge(df_cat, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "    .query(\"`amtallowed`.isna()\")\n",
    "    .merge(df_updated_ccmap, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "    .query(\"`updated_category`.isna()\")\n",
    "    .loc[:, ['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "    .drop_duplicates()\n",
    "    .assign(**{'class':'Acute'}, group='acute_ER')\n",
    ")\n",
    "\n",
    "\n",
    "# df_acute_er"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "366107a4e5b1e70c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_acute_noner = (\n",
    "    df_claims[['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'claimid']]\n",
    "    .merge(df_claimsip, on=['claimid'], how='left')\n",
    "    .query(\"`er_visitid`.isna()\")\n",
    "    .merge(df_acute_er, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "    .query(\"`group`.isna()\")\n",
    "    .loc[:, ['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "    .drop_duplicates()\n",
    "    .merge(df_cat, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "    .query(\"`amtallowed`.isna()\")\n",
    "    .merge(df_updated_ccmap, on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'], how='left')\n",
    "    .query(\"`updated_category`.isna()\")\n",
    "    .loc[:, ['tenantid', 'tag_tpa', 'tag_exchange', 'personid']]\n",
    "    .drop_duplicates()\n",
    "    .assign(**{'class':'Acute'}, group='acute_NonER')\n",
    ")\n",
    "\n",
    "# del df_acute\n",
    "# df_acute_noner = df_acute_noner[df_acute_noner['tenantid'] == 'VEQ-Nutcracker Therapeutics, Inc']\n",
    "# df_acute_noner"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf9fd2cf4ccb0077"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_personid_groups = pd.concat([df_cat_groups, df_ccgroups, df_acute_er, df_acute_noner], axis=0)\n",
    "# df_personid_groups"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf6e357971aaa614"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del df_cat_groups, df_ccgroups, df_acute_er, df_acute_noner"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d48cb68ef716cb4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Copy df_claims to df_dmca_groups and merge with df_claimsip\n",
    "df_dmca_groups = df_claims.copy().merge(df_claimsip, on=['claimid'], how='left')\n",
    "\n",
    "# Define the conditions\n",
    "conditions = [\n",
    "    (df_dmca_groups['claimtype'] == 'MED') & (df_dmca_groups['inpatient_visitid'].notna() | df_dmca_groups['servicecategory_details'].str.lower().str.contains('inpatient')),\n",
    "    (df_dmca_groups['claimtype'] == 'MED') & df_dmca_groups['er_visitid'].notna(),\n",
    "    (df_dmca_groups['claimtype'] == 'MED') & df_dmca_groups['servicecategory_details'].str.lower().str.contains('outpatient') & ~df_dmca_groups['servicecategory_details'].str.lower().isin(['outpatient - office', 'outpatient - mh/sa']) & df_dmca_groups['inpatient_visitid'].isna() & df_dmca_groups['er_visitid'].isna(),\n",
    "    (df_dmca_groups['claimtype'] == 'MED') & df_dmca_groups['servicecategory_details'].str.lower().str.contains('outpatient') & df_dmca_groups['servicecategory_details'].str.lower().isin(['outpatient - office', 'outpatient - mh/sa']) & df_dmca_groups['inpatient_visitid'].isna() & df_dmca_groups['er_visitid'].isna(),\n",
    "    (df_dmca_groups['claimtype'] == 'Rx')\n",
    "]\n",
    "\n",
    "# Define the outputs\n",
    "outputs = ['IP', 'ER', 'Outpatient', 'Office Visit', 'Rx']\n",
    "\n",
    "# Create 'service_type' column\n",
    "df_dmca_groups['service_type'] = np.select(conditions, outputs, default=np.nan)\n",
    "\n",
    "# Merge with df_personid_groups and select necessary columns\n",
    "df_dmca_groups = df_dmca_groups.merge(df_personid_groups, how='inner', on=['tenantid', 'tag_tpa', 'tag_exchange', 'personid'])\n",
    "df_dmca_groups = df_dmca_groups[['tenantid', 'tag_tpa', 'tag_exchange', 'personid', 'class', 'group', 'service_type', 'inpatient_visitid', 'er_visitid', 'tpaclaimid', 'amtallowed', 'amtpaid']]\n",
    "\n",
    "# Filter out 'nan' and create 'counts_field' column\n",
    "df_dmca_groups = df_dmca_groups[df_dmca_groups['service_type'] != 'nan']\n",
    "df_dmca_groups['counts_field'] = df_dmca_groups.apply(lambda row: row['inpatient_visitid'] if row['service_type'] == 'IP' else (row['er_visitid'] if row['service_type'] == 'ER' else row['tpaclaimid']), axis=1)\n",
    "\n",
    "# Group by 'tenantid', 'tag_tpa', 'tag_exchange', 'class', 'group' and calculate unique 'personid'\n",
    "df_claimants = df_dmca_groups.groupby(['tenantid', 'tag_tpa', 'tag_exchange', 'class', 'group'])['personid'].nunique().reset_index()\n",
    "\n",
    "# Group by 'tenantid', 'tag_tpa', 'tag_exchange', 'class', 'group', 'service_type' and calculate necessary values\n",
    "df_dmca_groups = df_dmca_groups.groupby(['tenantid', 'tag_tpa', 'tag_exchange', 'class', 'group', 'service_type']).agg({'counts_field':'nunique', 'amtallowed':'sum', 'amtpaid':'sum'}).reset_index()\n",
    "\n",
    "# Merge the two DataFrames\n",
    "df_dmca_groups = pd.merge(df_dmca_groups, df_claimants, on=['tenantid', 'tag_tpa', 'tag_exchange', 'class', 'group'])\n",
    "\n",
    "# Rename columns\n",
    "df_dmca_groups.rename(columns={'counts_field':'counts', 'amtallowed': 'allowed', 'amtpaid': 'paid', 'personid': 'claimants'}, inplace=True)\n",
    "\n",
    "# Add 'year', 'start_date', and 'stop_date' columns\n",
    "df_dmca_groups['year'] = pd.to_datetime(start_date).strftime('%Y')\n",
    "df_dmca_groups['start_date'] = start_date\n",
    "df_dmca_groups['stop_date'] = stop_date\n",
    "\n",
    "# Save to CSV file\n",
    "df_dmca_groups.to_csv(f'Claims Data/claims_{curr_month.replace(\"-\", \"_\")}.csv',index=False)\n",
    "\n",
    "# df_dmca_groups"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e870c71ab204ff2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cf49652b5b3cdc08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
